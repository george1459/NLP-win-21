ls
# install.packages("stm")
library(stm)
library(tidyverse)
library(gutenbergr)
library(dplyr)
sherlock_raw <- gutenberg_download(1661)
str_detect
sherlock <- sherlock_raw %>%
mutate(story = ifelse(str_detect(text, "ADVENTURE"),
text,
NA)) %>%
fill(story) %>%
filter(story != "THE ADVENTURES OF SHERLOCK HOLMES") %>%
mutate(story = factor(story, levels = unique(story)))
sherlock
sherlock
library(tidytext)
tidy_sherlock <- sherlock %>%
mutate(line = row_number()) %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
filter(word != "holmes")
sherlock_dfm
sherlock_dfm <- tidy_sherlock %>%
count(story, word, sort = TRUE) %>%
cast_dfm(story, word, n)
?stm
sherlock_dfm
sherlock_dfm
1
sherlock_dfm[1]
sherlock_dfm[3]
sherlock_dfm[0]
sherlock_dfm[[0]]
sherlock_dfm[[0]]
sherlock_dfm
# install.packages("stm")
library(stm)
library(tidyverse)
library(gutenbergr)
library(dplyr)
library(tidytext)
library(quanteda)
# library(rjson)
library(tm)
library(data.table)
setwd("/Users/admin/Desktop/NLP-win-21/jason")
data <- read.csv("/Users/admin/Desktop/NLP-data/sample_df-V1.csv")
# PREP
processed <- textProcessor(data$text, metadata = data)
out <- prepDocuments(processed$documents, processed$vocab, processed$meta,
lower.thresh = 10, upper.thresh = as.integer(n_docs / 2))
n_docs <- length(data[[2]])
n_topic <- 50
# PREP
processed <- textProcessor(data$text, metadata = data)
out <- prepDocuments(processed$documents, processed$vocab, processed$meta,
lower.thresh = 10, upper.thresh = as.integer(n_docs / 2))
docs <- out$documents
# vocab <- out$vocab
meta <-out$meta
View(docs)
length(docs)
# Estimate STM
stm1 <- stm(documents = out$documents, vocab = out$vocab,
K = n_topic, prevalence =~ s(date),
max.em.its = 20, data = out$meta,
init.type = "Spectral")
print(stm1$time)
vocab <- stm1$vocab
beta <- stm1$beta # log word prob in each topic
topic_word <- beta$logbeta[[1]]
topic_names = c(1:n_topic)
names(topic_names) = c(1:n_topic)
for (i in c(1:n_topic)) {
print(i)
topic_names[i] = paste(vocab[n_big_idx(topic_word[i,], n=n_topic)],
collapse = ' ')
}
# 找每个topic的words -- top 10 words
n_big_idx <- function(x, n=3) {
nx <- length(x)
p <- nx-n
xp <- sort(x, partial=p)[p]
which(x > xp)}
for (i in c(1:n_topic)) {
print(i)
topic_names[i] = paste(vocab[n_big_idx(topic_word[i,], n=n_topic)],
collapse = ' ')
}
for (i in c(1:n_topic)) {
# print(i)
topic_names[i] = paste(vocab[n_big_idx(topic_word[i,], n=n_topic)],
collapse = ' ')
}
topic_names <- list(topic_names)
topic_names
setDT(topic_names)
fwrite(list(topic_names[[1]]), file = paste("topic_names", trial_name))
trial_name <- "-test1"
fwrite(list(topic_names[[1]]), file = paste("topic_names", trial_name))
# 找每个document的topic
vocab <- stm1$vocab
theta <- stm1$theta # document-topic matrix
doc_topic <- theta
# doc_topic[1,]
doc_topic_out <- add_column(tibble(doc_topic), meta$date)
write.table(doc_topic_out , file = "doc_topic-test1.csv")
file.path
output_dir <-  'output'
file.path(output_dir, "doc_topic-test1.csv")
library("rjson")
filepath <- "/Users/admin/Desktop/NLP-data/RMRB_5_each_month.jsonlist.json"
con = file(filepath, "r")
while ( TRUE ) {
line = readLines(con, n = 1)
if ( length(line) == 0 ) {
break
}
json_data <- fromJSON(line)
json_data <- data.frame(json_data)
df <- rbind(df, json_data)
}
df <- data.frame()
filepath <- "/Users/admin/Desktop/NLP-data/RMRB_5_each_month.jsonlist.json"
con = file(filepath, "r")
while ( TRUE ) {
line = readLines(con, n = 1)
if ( length(line) == 0 ) {
break
}
json_data <- fromJSON(line)
json_data <- data.frame(json_data)
df <- rbind(df, json_data)
}
close(con)
close(con)
View(df)
data <- data.frame()
con = file(filepath, "r")
while ( TRUE ) {
line = readLines(con, n = 1)
if ( length(line) == 0 ) {
break
}
json_data <- fromJSON(line)
json_data <- data.frame(json_data)
df <- rbind(df, json_data)
}
trial_postfix <- "-test1.csv" # 输出两个文件的后缀
n_topic <- 50
output_dir <- '' # output directory name
n_docs <- length(data[[2]])
n_docs
# PREP
processed <- textProcessor(data$text, metadata = data)
data
data <- data.frame()
filepath <- "/Users/admin/Desktop/NLP-data/RMRB_5_each_month.jsonlist.json"
con = file(filepath, "r")
while ( TRUE ) {
line = readLines(con, n = 1)
if ( length(line) == 0 ) {
break
}
json_data <- fromJSON(line)
json_data <- data.frame(json_data)
df <- rbind(data, json_data)
}
close(con)
trial_postfix <- "-test1.csv" # 输出两个文件的后缀
n_topic <- 50
output_dir <- '' # output directory name
n_docs <- length(data[[2]])
n_docs
n_docs <- length(data[[2]])
nrows(data)
nrow(data)
dim(data)
data
d
data <- data.frame()
filepath <- "/Users/admin/Desktop/NLP-data/RMRB_5_each_month.jsonlist.json"
con = file(filepath, "r")
while ( TRUE ) {
line = readLines(con, n = 1)
if ( length(line) == 0 ) {
break
}
json_data <- fromJSON(line)
json_data <- data.frame(json_data)
data <- rbind(data, json_data)
}
close(con)
trial_postfix <- "-test1.csv" # 输出两个文件的后缀
n_topic <- 50
output_dir <- '' # output directory name
n_docs <- length(data[[2]])
n_docs
# PREP
processed <- textProcessor(data$text, metadata = data)
out <- prepDocuments(processed$documents, processed$vocab, processed$meta,
lower.thresh = 5, upper.thresh = as.integer(n_docs / 2))
